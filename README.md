# Blog-Crawler
##爬虫实现原理
###网络爬虫的整体执行流程：
1） 确定一个（多个）种子网页<br>
2） 进行数据的内容提取<br>
3） 将网页中的关联网页连接提取出来<br>
4） 将尚未爬取的关联网页内容放到一个队列中<br>
5） 从队列中取出一个待爬取的页面，判断之前是否爬过。<br>
6） 把没有爬过的进行爬取，并进行之前的重复操作。<br>
7） 直到队列中没有新的内容，爬虫执行结束。<br>

###完成爬虫时，会有一些概念必须知道的：<br>
1） 深度（depth）：一般来说，表示从种子页到当前页的打开连接数，一般建议不要超过5层。<br>
2） 广度（宽度）优先和深度优先：表示爬取时的优先级。建议使用广度优先，按深度的层级来顺序爬取。<br>
###接下来可以在网页的数据上下手了。
先实现一个单网页数据提取的功能。<br>
使用的技术可以有以下几类：<br>
1） 原生代码实现：<br>
　　a) URL类<br>
2） 使用第三方的URL库<br>
　　a) HttpClient库<br>
3） 开源爬虫框架<br>
　　a) Heritrix<br>
　　b) Nutch<br>
![]()
下面尝试将这个网页的源代码保存成为本地的一个文本文件，以便后续做离线分析。<br>
如果想根据条件提取网页中的内容信息，那么就需要使用Java的正则表达式。<br>
正则表达式Java.util包下提供了Pattern和Matcher这两个类，可以根据我们给定的条件来进行数据的匹配和提取。<br>
通过Pattern类中提供的规则字符或字符串，我们需要自己拼凑出我们的匹配规则。<br>
正则表达式最常用的地方是用来做表单提交的数据格式验证的。<br><br>
通过正则表达式完成超连接的连接匹配和提取对爬取的HTML页面来说，如果想提取连接地址，就必须找到所有超连接的标签和对应的属性。<br>

超连接标签是<a></a>，保存连接的属性是：href。<br>
<a href=”…”>…</a> <br>
规则：<br>
<a .*href=.+</a> <br>
##控制台显示










